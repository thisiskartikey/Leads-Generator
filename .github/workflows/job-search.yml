name: Job Radar Search & Analysis

# Triggers: Tuesday & Saturday at 8am UTC, or manual
on:
  schedule:
    # Run Tuesday (2) and Saturday (6) at 8:00 AM UTC
    # Adjust to your timezone: UTC+0 = 8am UTC, UTC-5 = 3am EST, UTC-8 = 12am PST
    - cron: '0 8 * * 2,6'

  # Allow manual trigger from Actions tab
  workflow_dispatch:

# Permissions needed to commit results back to repo
permissions:
  contents: write

jobs:
  search-and-analyze:
    runs-on: ubuntu-latest

    steps:
      # Step 1: Checkout repository
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for potential debugging

      # Step 2: Setup Python environment
      - name: Setup Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'  # Cache pip dependencies for faster runs

      # Step 3: Install dependencies
      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      # Step 4: Run Job Radar
      - name: Run Job Radar Search & Analysis
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          SERPAPI_KEY: ${{ secrets.SERPAPI_KEY }}
        run: |
          cd src
          python main.py

      # Step 5: Commit and push results
      - name: Commit updated job results
        run: |
          git config user.name "Job Radar Bot"
          git config user.email "actions@github.com"

          # Add data files
          git add data/results.json data/history.json

          # Check if there are changes to commit
          if git diff --staged --quiet; then
            echo "No changes to commit"
          else
            # Commit with timestamp
            git commit -m "Update job results - $(date -u '+%Y-%m-%d %H:%M UTC')"

            # Push to main branch
            git push
          fi

      # Step 6: Create summary
      - name: Generate workflow summary
        if: always()
        run: |
          echo "## Job Radar Execution Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Timestamp:** $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Check if results.json exists and extract stats
          if [ -f "data/results.json" ]; then
            echo "**Results:**" >> $GITHUB_STEP_SUMMARY
            python3 << 'EOF' >> $GITHUB_STEP_SUMMARY
          import json
          import sys

          try:
              with open('data/results.json', 'r') as f:
                  data = json.load(f)

              metadata = data.get('metadata', {})
              jobs = data.get('jobs', [])

              print(f"- Total searched: {metadata.get('total_searched', 0)}")
              print(f"- New jobs found: {metadata.get('new_jobs_found', 0)}")
              print(f"- Jobs analyzed: {metadata.get('jobs_analyzed', 0)}")
              print(f"- Jobs in results: {len(jobs)}")

              # Count by category
              ai_jobs = sum(1 for job in jobs if job.get('ai_analysis', {}).get('fit_score', 0) >= job.get('sustainability_analysis', {}).get('fit_score', 0))
              sus_jobs = len(jobs) - ai_jobs

              print(f"- AI/Tech jobs: {ai_jobs}")
              print(f"- Sustainability jobs: {sus_jobs}")

              # High fit jobs
              high_fit = sum(1 for job in jobs if max(
                  job.get('ai_analysis', {}).get('fit_score', 0),
                  job.get('sustainability_analysis', {}).get('fit_score', 0)
              ) >= 90)

              print(f"- High fit jobs (90+): {high_fit}")

          except Exception as e:
              print(f"Error reading results: {e}")
          EOF
          else
            echo "âŒ No results file found" >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Dashboard:** Check [GitHub Pages](https://yourusername.github.io/job-radar/) for updated results" >> $GITHUB_STEP_SUMMARY
